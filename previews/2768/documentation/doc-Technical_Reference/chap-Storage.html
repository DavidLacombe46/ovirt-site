<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>oVirt | oVirt is a free open-source virtualization solution for your entire enterprise</title>
<meta name="generator" content="Jekyll v3.9.1" />
<meta property="og:title" content="oVirt" />
<meta property="og:locale" content="en" />
<meta name="description" content="oVirt is a free open-source virtualization solution for your entire enterprise" />
<meta property="og:description" content="oVirt is a free open-source virtualization solution for your entire enterprise" />
<link rel="canonical" href="https://ovirt.github.io/ovirt-site/previews/2768/documentation/doc-Technical_Reference/chap-Storage.html" />
<meta property="og:url" content="https://ovirt.github.io/ovirt-site/previews/2768/documentation/doc-Technical_Reference/chap-Storage.html" />
<meta property="og:site_name" content="oVirt" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="oVirt" />
<meta name="twitter:site" content="@ovirt" />
<script type="application/ld+json">
{"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://ovirt.github.io/ovirt-site/previews/2768/images/logo.svg"}},"description":"oVirt is a free open-source virtualization solution for your entire enterprise","@type":"WebPage","headline":"oVirt","url":"https://ovirt.github.io/ovirt-site/previews/2768/documentation/doc-Technical_Reference/chap-Storage.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  <meta name="viewport" content="initial-scale=1.0,user-scalable=no,maximum-scale=1,width=device-width">
  <link rel="stylesheet" href="/ovirt-site/previews/2768/stylesheets/fonts.css" />
  <link rel='stylesheet' href='https://fonts.googleapis.com/css?family=Open+Sans'>
  <link rel="stylesheet" href="/ovirt-site/previews/2768/stylesheets/application.css" />
  <link rel="stylesheet" href="/ovirt-site/previews/2768/stylesheets/print.css" media="print" />
  <link rel="stylesheet" href="/ovirt-site/previews/2768/stylesheets/coderay.css" media="screen" />
  <link rel="stylesheet" href="/ovirt-site/previews/2768/stylesheets/asciidoc.css" />
  <script src="/ovirt-site/previews/2768/javascripts/vendor/jquery.js" type="text/javascript"></script>
  <script src="/ovirt-site/previews/2768/javascripts/vendor/bootstrap.min.js" type="text/javascript"></script>
  <script src="/ovirt-site/previews/2768/javascripts/vendor/bootstrap-sortable.js" type="text/javascript"></script>
  <script src="/ovirt-site/previews/2768/javascripts/vendor/moment.js" type="text/javascript"></script>
  <script src="/ovirt-site/previews/2768/javascripts/vendor/fullcalendar/fullcalendar.js" type="text/javascript"></script>
  <script src="/ovirt-site/previews/2768/javascripts/lib/cal-widget.js" type="text/javascript"></script>
  <!-- Begin Jekyll Favicon tag v0.2.9 -->
<!-- Classic -->
<link rel="shortcut icon" href="/ovirt-site/previews/2768/favicon.ico">
<link rel="icon" sizes="48x48 32x32 16x16" href="/ovirt-site/previews/2768/favicon.ico">
  <link rel="icon" sizes="16x16" type="image/png" href="/ovirt-site/previews/2768/images/favicon-16x16.png">
  <link rel="icon" sizes="32x32" type="image/png" href="/ovirt-site/previews/2768/images/favicon-32x32.png">
  <link rel="icon" sizes="64x64" type="image/png" href="/ovirt-site/previews/2768/images/favicon-64x64.png">
  <link rel="icon" sizes="144x144" type="image/png" href="/ovirt-site/previews/2768/images/favicon-144x144.png">
<!-- Safari -->
  <link rel="apple-touch-icon" sizes="57x57" href="/ovirt-site/previews/2768/images/favicon-57x57.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/ovirt-site/previews/2768/images/favicon-76x76.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/ovirt-site/previews/2768/images/favicon-120x120.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/ovirt-site/previews/2768/images/favicon-152x152.png">
  <link rel="apple-touch-icon" sizes="167x167" href="/ovirt-site/previews/2768/images/favicon-167x167.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/ovirt-site/previews/2768/images/favicon-180x180.png">
<!-- Chrome -->
  <link rel="icon" sizes="192x192" href="/ovirt-site/previews/2768/images/favicon-192x192.png">
  <link rel="icon" sizes="96x96" href="/ovirt-site/previews/2768/images/favicon-96x96.png">
  <link rel="icon" sizes="48x48" href="/ovirt-site/previews/2768/images/favicon-48x48.png">
<link rel="manifest" href="/ovirt-site/previews/2768/manifest.webmanifest">
<!-- IE -->
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/ovirt-site/previews/2768/images/favicon-144x144.png">
<meta name='msapplication-config' content="/ovirt-site/previews/2768/browserconfig.xml">
<!-- End Jekyll Favicon tag -->
</head>
<body class=""><header class='masthead hidden-print' id='branding' role='banner'><section class='hgroup'></section><div id='access'><nav id="mainNav" class="navbar navbar-fixed-top affix-top">  <div class="container">    <div class="col-sm-2 navbar-header">      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#menu_id0980fddf">        <span class="sr-only">Toggle navigation</span>        <span>Menu</span>        <span class="fa fa-bars"></span>      </button>      <a class="navbar-brand" href="/ovirt-site/previews/2768/">        <img id="logo" alt="oVirt" src="/ovirt-site/previews/2768/images/logo.svg">      </a>    </div>    <!-- Collect the nav links, forms, and other content for toggling -->    <div class="col-sm-10">      <div class="navbar-collapse collapse" id="menu_id0980fddf">        <ul class="nav navbar-nav">          <li class="hidden active">            <a href="#page-wrap"></a>          </li><li role='menuitem'>  <a href='/ovirt-site/previews/2768/download/'>Download</a></li><li class='active' role='menuitem'>  <a href='/ovirt-site/previews/2768/documentation/'>Documentation</a></li><li role='menuitem'>  <a href='/ovirt-site/previews/2768/develop/'>Developers</a></li><li role='menuitem'>  <a href='/ovirt-site/previews/2768/community/'>Community</a></li><li role='menuitem'>  <a href='/ovirt-site/previews/2768//lists.ovirt.org/archives/'>Forum</a></li><li role='menuitem'>  <a href='/ovirt-site/previews/2768//blogs.ovirt.org/'>Blog</a></li>              </ul>          </div>          <!-- /.navbar-collapse -->      </div>      <!-- /.container-fluid -->    </div>  </div></nav></div></header><main id="page-wrap" class="page-wrap" aria-label="Content">
      <section id="page" class="page">
        <!-- adapted from https://github.com/git-no/jekyll-breadcrumbs -->
<nav class="breadcrumbs bootstrap hidden-sm-down" aria-label="breadcrumb">

   <ol class="breadcrumb list-unstyled" vocab="http://schema.org/" typeof="BreadcrumbList">

      
        
        
        <li class="breadcrumb-item" property="itemListElement" typeof="ListItem">
           <a property="item" typeof="WebPage" href="/ovirt-site/previews/2768/">
              <span property="name"></span>
              <meta property="position" content="1" />
           </a>
        </li>
      
        
        
        <li class="breadcrumb-item" property="itemListElement" typeof="ListItem">
           <a property="item" typeof="WebPage" href="/ovirt-site/previews/2768/documentation/">
              <span property="name">Documentation</span>
              <meta property="position" content="2" />
           </a>
        </li>
      
        
        
        <li class="breadcrumb-item" property="itemListElement" typeof="ListItem">
           <a property="item" typeof="WebPage" href="/ovirt-site/previews/2768/documentation/doc-Technical_Reference/">
              <span property="name">Technical Reference</span>
              <meta property="position" content="3" />
           </a>
        </li>
      
        
        
          

   </ol>

</nav>

        
          
        
        <section id="content" class="content container">
          
<div class="sect1">
<h2 id="chap-Storage">Storage</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="Storage_Domains1">Storage Domains Overview</h3>
<div class="paragraph">
<p>A storage domain is a collection of images that have a common storage interface. A storage domain contains complete images of templates and virtual machines (including snapshots), ISO files, and metadata about themselves. A storage domain can be made of either block devices (SAN - iSCSI or FCP) or a file system (NAS - NFS, GlusterFS, or other POSIX compliant file systems).</p>
</div>
<div class="paragraph">
<p>On NAS, all virtual disks, templates, and snapshots are files.</p>
</div>
<div class="paragraph">
<p>On SAN (iSCSI/FCP), each virtual disk, template or snapshot is a logical volume. Block devices are aggregated into a logical entity called a volume group, and then divided by LVM (Logical Volume Manager) into logical volumes for use as virtual hard disks. See the <a href="{URL_rhel_docs_legacy}html-single/Logical_Volume_Manager_Administration/index.html">{enterprise-linux} Logical Volume Manager Administration Guide</a> for more information on LVM.</p>
</div>
<div class="paragraph">
<p>Virtual disks can have one of two formats, either QCOW2 or raw. The type of storage can be either sparse or preallocated. Snapshots are always sparse but can be taken for disks of either format.</p>
</div>
<div class="paragraph">
<p>Virtual machines that share the same storage domain can be migrated between hosts that belong to the same cluster.</p>
</div>
</div>
<div class="sect2">
<h3 id="Types_Of_Storage_Backing_Storage_Domains">Types of Storage Backing Storage Domains</h3>
<div class="paragraph">
<p>Storage domains can be implemented using block based and file based storage.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><strong>File Based Storage</strong></dt>
<dd>
<p>The file based storage types supported by {virt-product-fullname} are NFS, GlusterFS, other POSIX compliant file systems, and storage local to hosts.</p>
<div class="paragraph">
<p>File based storage is managed externally to the {virt-product-fullname} environment.</p>
</div>
<div class="paragraph">
<p>NFS storage is managed by a {enterprise-linux} NFS server, or other third party network attached storage server.</p>
</div>
<div class="paragraph">
<p>Hosts can manage their own local storage file systems.</p>
</div>
</dd>
<dt class="hdlist1"><strong>Block Based Storage</strong></dt>
<dd>
<p>Block storage uses unformatted block devices. Block devices are aggregated into volume groups by the Logical Volume Manager (LVM). An instance of LVM runs on all hosts, unaware of the instances running on other hosts. VDSM adds clustering logic on top of LVM by scanning volume groups for changes. When changes are detected, VDSM updates individual hosts by telling them to refresh their volume group information. The hosts divide the volume group into logical volumes, writing logical volume metadata to disk. If more storage capacity is added to an existing storage domain, the {virt-product-fullname} {engine-name} causes VDSM on each host to refresh volume group information.</p>
<div class="paragraph">
<p>A Logical Unit Number (LUN) is an individual block device. One of the supported block storage protocols, iSCSI or Fibre Channel, is used to connect to a LUN. The Red Hat Virtualization Manager manages software iSCSI connections to the LUNs. All other block storage connections are managed externally to the Red Hat Virtualization environment. Any changes in a block based storage environment, such as the creation of logical volumes, extension or deletion of logical volumes and the addition of a new LUN are handled by LVM on a specially selected host called the Storage Pool Manager. Changes are then synced by VDSM which storage metadata refreshes across all hosts in the cluster.</p>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="Storage_Domain_Types1">Storage Domain Types</h3>
<div class="paragraph">
<p>{virt-product-fullname} supports the following types of storage domains, as well as the storage types that each storage domain supports.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <strong>Data Domain</strong> stores the hard disk images of all virtual machines in the {virt-product-fullname} environment. Disk images may contain an installed operating system or data stored or generated by a virtual machine. Data storage domains support NFS, iSCSI, FCP, GlusterFS and POSIX compliant storage. A data domain cannot be shared between multiple data centers.</p>
</li>
<li>
<p>The <strong>Export Domain</strong> provides transitory storage for hard disk images and virtual machine templates being transferred between data centers. Additionally, export storage domains store backed up copies of virtual machines. Export storage domains support NFS storage. Multiple data centers can access a single export storage domain but only one data center can use it at a time.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The Export domain is deprecated. Storage data domains can be unattached from a data center and imported to another data center in the same environment, or in a different environment. Virtual machines, floating virtual disks, and templates can then be uploaded from the imported storage domain to the attached data center.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>The <strong>ISO Domain</strong> stores ISO files, also called images. ISO files are representations of physical CDs or DVDs. In the {virt-product-fullname} environment the common types of ISO files are operating system installation disks, application installation disks, and guest agent installation disks. These images can be attached to virtual machines and booted in the same way that physical disks are inserted into a disk drive and booted. ISO storage domains allow all hosts within the data center to share ISOs, eliminating the need for physical optical media.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The ISO domain is a deprecated storage domain type. The ISO Uploader tool has been deprecated. Red Hat recommends uploading ISO images to the data domain with the Administration Portal or with the REST API.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="QCOW2">Storage Formats for Virtual Disks</h3>
<div class="dlist">
<dl>
<dt class="hdlist1"><strong>QCOW2 Formatted Virtual Machine Storage</strong></dt>
<dd>
<p>QCOW2 is a storage format for virtual disks. QCOW stands for QEMU copy-on-write. The QCOW2 format decouples the physical storage layer from the virtual layer by adding a mapping between logical and physical blocks. Each logical block is mapped to its physical offset, which enables storage over-commitment and virtual machine snapshots, where each QCOW volume only represents changes made to an underlying virtual disk.</p>
<div class="paragraph">
<p>The initial mapping points all logical blocks to the offsets in the backing file or volume. When a virtual machine writes data to a QCOW2 volume after a snapshot, the relevant block is read from the backing volume, modified with the new information and written into a new snapshot QCOW2 volume. Then the map is updated to point to the new place.</p>
</div>
</dd>
<dt class="hdlist1"><strong>Raw</strong></dt>
<dd>
<p>The raw storage format has a performance advantage over QCOW2 in that no formatting is applied to virtual disks stored in the raw format. Virtual machine data operations on virtual disks stored in raw format require no additional work from hosts. When a virtual machine writes data to a given offset in its virtual disk, the I/O is written to the same offset on the backing file or logical volume.</p>
<div class="paragraph">
<p>Raw format requires that the entire space of the defined image be preallocated unless using externally managed thin provisioned LUNs from a storage array.</p>
</div>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="Preallocated_Storage">Virtual Disk Storage Allocation Policies</h3>
<div class="dlist">
<dl>
<dt class="hdlist1"><strong>Preallocated Storage</strong></dt>
<dd>
<p>All of the storage required for a virtual disk is allocated prior to virtual machine creation. If a 20 GB disk image is created for a virtual machine, the disk image uses 20 GB of storage domain capacity. Preallocated disk images cannot be enlarged. Preallocating storage can mean faster write times because no storage allocation takes place during runtime, at the cost of flexibility. Allocating storage this way reduces the capacity of the {virt-product-fullname} {engine-name} to overcommit storage. Preallocated storage is recommended for virtual machines used for high intensity I/O tasks with less tolerance for latency in storage. Generally, server virtual machines fit this description.</p>
</dd>
</dl>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If thin provisioning functionality provided by your storage back-end is being used, preallocated storage should still be selected from the Administration Portal when provisioning storage for virtual machines.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><strong>Sparsely Allocated Storage</strong></dt>
<dd>
<p>The upper size limit for a virtual disk is set at virtual machine creation time. Initially, the disk image does not use any storage domain capacity. Usage grows as the virtual machine writes data to disk, until the upper limit is reached. Capacity is not returned to the storage domain when data in the disk image is removed. Sparsely allocated storage is appropriate for virtual machines with low or medium intensity I/O tasks with some tolerance for latency in storage. Generally, desktop virtual machines fit this description.</p>
</dd>
</dl>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If thin provisioning functionality is provided by your storage back-end, it should be used as the preferred implementation of thin provisioning. Storage should be provisioned from the graphical user interface as preallocated, leaving thin provisioning to the back-end solution.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="Storage_metadata_versions_in_Red_Hat_Enterprise_Virtualization">Storage Metadata Versions in {virt-product-fullname}</h3>
<div class="paragraph">
<p>{virt-product-fullname} stores information about storage domains as metadata on the storage domains themselves. Each major release of {virt-product-fullname} has seen improved implementations of storage metadata.</p>
</div>
<div class="ulist">
<div class="title">V1 metadata ({virt-product-fullname} 2.x series)</div>
<ul>
<li>
<p>Each storage domain contains metadata describing its own structure, and all of the names of physical volumes that are used to back virtual disks.</p>
</li>
<li>
<p>Master domains additionally contain metadata for all the domains and physical volume names in the storage pool. The total size of this metadata is limited to 2 KB, limiting the number of storage domains that can be in a pool.</p>
</li>
<li>
<p>Template and virtual machine base images are read only.</p>
</li>
<li>
<p>V1 metadata is applicable to NFS, iSCSI, and FC storage domains.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">V2 metadata (Red Hat Enterprise Virtualization 3.0)</div>
<ul>
<li>
<p>All storage domain and pool metadata is stored as logical volume tags rather than written to a logical volume. Metadata about virtual disk volumes is still stored in a logical volume on the domains.</p>
</li>
<li>
<p>Physical volume names are no longer included in the metadata.</p>
</li>
<li>
<p>Template and virtual machine base images are read only.</p>
</li>
<li>
<p>V2 metadata is applicable to iSCSI, and FC storage domains.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">V3 metadata (Red Hat Enterprise Virtualization 3.1 and later)</div>
<ul>
<li>
<p>All storage domain and pool metadata is stored as logical volume tags rather than written to a logical volume. Metadata about virtual disk volumes is still stored in a logical volume on the domains.</p>
</li>
<li>
<p>Virtual machine and template base images are no longer read only. This change enables live snapshots, live storage migration, and clone from snapshot.</p>
</li>
<li>
<p>Support for unicode metadata is added, for non-English volume names.</p>
</li>
<li>
<p>V3 metadata is applicable to NFS, GlusterFS, POSIX, iSCSI, and FC storage domains.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">V4 metadata (Red Hat Virtualization 4.1 and later)</div>
<ul>
<li>
<p>Support for QCOW2 compat levels - the QCOW image format includes a version number to allow introducing new features that change the image format so that it is incompatible with earlier versions. Newer QEMU versions (1.7 and above) support QCOW2 version 3, which is not backwards compatible, but introduces improvements such as zero clusters and improved performance.</p>
</li>
<li>
<p>A new xleases volume to support VM leases - this feature adds the ability to acquire a lease per virtual machine on shared storage without attaching the lease to a virtual machine disk.</p>
<div class="paragraph">
<p>A VM lease offers two important capabilities:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Avoiding split-brain.</p>
</li>
<li>
<p>Starting a VM on another host if the original host becomes non-responsive, which improves the availability of HA VMs.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">V5 metadata (Red Hat Virtualization 4.3 and later)</div>
<ul>
<li>
<p>Support for 4K (4096 byte) block storage.</p>
</li>
<li>
<p>Support for variable SANLOCK allignments.</p>
</li>
<li>
<p>Support for new properties:</p>
<div class="ulist">
<ul>
<li>
<p><code>BLOCK_SIZE</code> - stores the block size of the storage domain in bytes.</p>
</li>
<li>
<p><code>ALIGNMENT</code> -  determines the formatting and size of the xlease volume. (1MB to 8MB).
Determined by the maximum number of host to be supported (value provided by the
user) and disk block size.</p>
<div class="paragraph">
<p>For example: a 512b block size and support for 2000 hosts
results in a 1MB xlease volume.</p>
</div>
<div class="paragraph">
<p>A 4K block size with 2000 hosts
results in a 8MB xlease volume.</p>
</div>
<div class="paragraph">
<p>The default value of maximum hosts is 250, resulting in  an xlease volume of 1MB for 4K disks.</p>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Deprecated properties:</p>
<div class="ulist">
<ul>
<li>
<p>The <code>LOGBLKSIZE</code>, <code>PHYBLKSIZE</code>, <code>MTIME</code>, and <code>POOL_UUID</code> fields were removed from the storage domain metadata.</p>
</li>
<li>
<p>The <code>SIZE</code> (size in blocks) field was replaced by <code>CAP</code> (size in bytes).</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="ulist">
<ul>
<li>
<p>You cannot boot from a 4K format disk, as the boot disk always uses a 512 byte emulation.</p>
</li>
<li>
<p>The nfs format always uses 512 bytes.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="Storage_Domain_Autorecovery_in_Red_Hat_Enterprise_Virtualization">Storage Domain Autorecovery in {virt-product-fullname}</h3>
<div class="paragraph">
<p>Hosts in a {virt-product-fullname} environment monitor storage domains in their data centers by reading metadata from each domain. A storage domain becomes inactive when all hosts in a data center report that they cannot access the storage domain.</p>
</div>
<div class="paragraph">
<p>Rather than disconnecting an inactive storage domain, the {engine-name} assumes that the storage domain has become inactive temporarily, because of a temporary network outage for example. Once every 5 minutes, the {engine-name} attempts to re-activate any inactive storage domains.</p>
</div>
<div class="paragraph">
<p>Administrator intervention may be required to remedy the cause of the storage connectivity interruption, but the {engine-name} handles re-activating storage domains as connectivity is restored.</p>
</div>
</div>
<div class="sect2">
<h3 id="Role_The_Storage_Pool_Manager">The Storage Pool Manager</h3>
<div class="paragraph">
<p>{virt-product-fullname} uses metadata to describe the internal structure of storage domains. Structural metadata is written to a segment of each storage domain. Hosts work with the storage domain metadata based on a single writer, and multiple readers configuration. Storage domain structural metadata tracks image and snapshot creation and deletion, and volume and domain extension.</p>
</div>
<div class="paragraph">
<p>The host that can make changes to the structure of the data domain is known as the Storage Pool Manager (SPM). The SPM coordinates all metadata changes in the data center, such as creating and deleting disk images, creating and merging snapshots, copying images between storage domains, creating templates and storage allocation for block devices. There is one SPM for every data center. All other hosts can only read storage domain structural metadata.</p>
</div>
<div class="paragraph">
<p>A host can be manually selected as the SPM, or it can be assigned by the {virt-product-fullname} {engine-name}. The {engine-name} assigns the SPM role by causing a potential SPM host to attempt to assume a storage-centric lease. The lease allows the SPM host to write storage metadata. It is storage-centric because it is written to the storage domain rather than being tracked by the {engine-name} or hosts. Storage-centric leases are written to a special logical volume in the master storage domain called <strong>leases</strong>. Metadata about the structure of the data domain is written to a special logical volume called <strong>metadata</strong>. The <strong>leases</strong> logical volume protects the <strong>metadata</strong> logical volume from changes.</p>
</div>
<div class="paragraph">
<p>The {engine-name} uses VDSM to issue the <strong>spmStart</strong> command to a host, causing VDSM on that host to attempt to assume the storage-centric lease. If the host is successful it becomes the SPM and retains the storage-centric lease until the {virt-product-fullname} {engine-name} requests that a new host assume the role of SPM.</p>
</div>
<div class="paragraph">
<p>The {engine-name} moves the SPM role to another host if:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The SPM host can not access all storage domains, but can access the master storage domain</p>
</li>
<li>
<p>The SPM host is unable to renew the lease because of a loss of storage connectivity or the lease volume is full and no write operation can be performed</p>
</li>
<li>
<p>The SPM host crashes</p>
</li>
</ul>
</div>
<div class="imageblock">
<div class="content">
<img src="992.png" alt="992">
</div>
<div class="title">Figure 1. The Storage Pool Manager Exclusively Writes Structural Metadata.</div>
</div>
</div>
<div class="sect2">
<h3 id="Storage_Pool_Manager_Selection_Process">Storage Pool Manager Selection Process</h3>
<div class="paragraph">
<p>If a host has not been manually assigned the Storage Pool Manager (SPM) role, the SPM selection process is initiated and managed by the {virt-product-fullname} {engine-name}.</p>
</div>
<div class="paragraph">
<p>First, the {virt-product-fullname} {engine-name} requests that VDSM confirm which host has the storage-centric lease.</p>
</div>
<div class="paragraph">
<p>The {virt-product-fullname} {engine-name} tracks the history of SPM assignment from the initial creation of a storage domain onward. The availability of the SPM role is confirmed in three ways:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The "getSPMstatus" command: the {engine-name} uses VDSM to check with the host that had SPM status last and receives one of "SPM", "Contending", or "Free".</p>
</li>
<li>
<p>The metadata volume for a storage domain contains the last host with SPM status.</p>
</li>
<li>
<p>The metadata volume for a storage domain contains the version of the last host with SPM status.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>If an operational, responsive host retains the storage-centric lease, the {virt-product-fullname} {engine-name} marks that host SPM in the administrator portal. No further action is taken.</p>
</div>
<div class="paragraph">
<p>If the SPM host does not respond, it is considered unreachable. If power management has been configured for the host, it is automatically fenced. If not, it requires manual fencing. The Storage Pool Manager role cannot be assigned to a new host until the previous Storage Pool Manager is fenced.</p>
</div>
<div class="paragraph">
<p>When the SPM role and storage-centric lease are free, the {virt-product-fullname} {engine-name} assigns them to a randomly selected operational host in the data center.</p>
</div>
<div class="paragraph">
<p>If the SPM role assignment fails on a new host, the {virt-product-fullname} {engine-name} adds the host to a list containing hosts the operation has failed on, marking these hosts as ineligible for the SPM role. This list is cleared at the beginning of the next SPM selection process so that all hosts are again eligible.</p>
</div>
<div class="paragraph">
<p>The {virt-product-fullname} {engine-name} continues request that the Storage Pool Manager role and storage-centric lease be assumed by a randomly selected host that is not on the list of failed hosts until the SPM selection succeeds.</p>
</div>
<div class="paragraph">
<p>Each time the current SPM is unresponsive or unable to fulfill its responsibilities, the {virt-product-fullname} {engine-name} initiates the Storage Pool Manager selection process.</p>
</div>
</div>
<div class="sect2">
<h3 id="Exclusive_Resources_and_Sanlock_in_Red_Hat_Enterprise_Virtualization">Exclusive Resources and Sanlock in {virt-product-fullname}</h3>
<div class="paragraph">
<p>Certain resources in the {virt-product-fullname} environment must be accessed exclusively.</p>
</div>
<div class="paragraph">
<p>The SPM role is one such resource. If more than one host were to become the SPM, there would be a risk of data corruption as the same data could be changed from two places at once.</p>
</div>
<div class="paragraph">
<p>Prior to Red Hat Enterprise Virtualization 3.1, SPM exclusivity was maintained and tracked using a VDSM feature called <strong>safelease</strong>. The lease was written to a special area on all of the storage domains in a data center. All of the hosts in an environment could track SPM status in a network-independent way. The VDSM&#8217;s safe lease only maintained exclusivity of one resource: the SPM role.</p>
</div>
<div class="paragraph">
<p>Sanlock provides the same functionality, but treats the SPM role as one of the resources that can be locked. Sanlock is more flexible because it allows additional resources to be locked.</p>
</div>
<div class="paragraph">
<p>Applications that require resource locking can register with Sanlock. Registered applications can then request that Sanlock lock a resource on their behalf, so that no other application can access it. For example, instead of VDSM locking the SPM status, VDSM now requests that Sanlock do so.</p>
</div>
<div class="paragraph">
<p>Locks are tracked on disk in a <strong>lockspace</strong>. There is one lockspace for every storage domain. In the case of the lock on the SPM resource, each host&#8217;s liveness is tracked in the lockspace by the host&#8217;s ability to renew the hostid it received from the {engine-name} when it connected to storage, and to write a timestamp to the lockspace at a regular interval. The <strong>ids</strong> logical volume tracks the unique identifiers of each host, and is updated every time a host renews its hostid. The SPM resource can only be held by a live host.</p>
</div>
<div class="paragraph">
<p>Resources are tracked on disk in the <strong>leases</strong> logical volume. A resource is said to be <strong>taken</strong> when its representation on disk has been updated with the unique identifier of the process that has taken it. In the case of the SPM role, the SPM resource is updated with the hostid that has taken it.</p>
</div>
<div class="paragraph">
<p>The Sanlock process on each host only needs to check the resources once to see that they are taken. After an initial check, Sanlock can monitor the lockspaces until timestamp of the host with a locked resource becomes stale.</p>
</div>
<div class="paragraph">
<p>Sanlock monitors the applications that use resources. For example, VDSM is monitored for SPM status and hostid. If the host is unable to renew it&#8217;s hostid from the {engine-name}, it loses exclusivity on all resources in the lockspace. Sanlock updates the resource to show that it is no longer taken.</p>
</div>
<div class="paragraph">
<p>If the SPM host is unable to write a timestamp to the lockspace on the storage domain for a given amount of time, the host&#8217;s instance of Sanlock requests that the VDSM process release its resources. If the VDSM process responds, its resources are released, and the SPM resource in the lockspace can be taken by another host.</p>
</div>
<div class="paragraph">
<p>If VDSM on the SPM host does not respond to requests to release resources, Sanlock on the host kills the VDSM process. If the kill command is unsuccessful, Sanlock escalates by attempting to kill VDSM using sigkill. If the sigkill is unsuccessful, Sanlock depends on the <strong>watchdog daemon</strong> to reboot the host.</p>
</div>
<div class="paragraph">
<p>Every time VDSM on the host renews its hostid and writes a timestamp to the lockspace, the watchdog daemon receives a <strong>pet</strong>. When VDSM is unable to do so, the watchdog daemon is no longer being petted. After the watchdog daemon has not received a pet for a given amount of time, it reboots the host. This final level of escalation, if reached, guarantees that the SPM resource is released, and can be taken by another host.</p>
</div>
</div>
<div class="sect2">
<h3 id="Over-commitment">Thin Provisioning and Storage Over-Commitment</h3>
<div class="paragraph">
<p>The {virt-product-fullname} {engine-name} provides provisioning policies to optimize storage usage within the virtualization environment. A thin provisioning policy allows you to over-commit storage resources, provisioning storage based on the actual storage usage of your virtualization environment.</p>
</div>
<div class="paragraph">
<p>Storage over-commitment is the allocation of more storage to virtual machines than is physically available in the storage pool. Generally, virtual machines use less storage than what has been allocated to them. Thin provisioning allows a virtual machine to operate as if the storage defined for it has been completely allocated, when in fact only a fraction of the storage has been allocated.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>While the {virt-product-fullname} {engine-name} provides its own thin provisioning function, you should use the thin provisioning functionality of your storage back-end if it provides one.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>To support storage over-commitment, VDSM defines a threshold which compares logical storage allocation with actual storage usage. This threshold is used to make sure that the data written to a disk image is smaller than the logical volume that backs the disk image. QEMU identifies the highest offset written to in a logical volume, which indicates the point of greatest storage use. VDSM monitors the highest offset marked by QEMU to ensure that the usage does not cross the defined threshold. So long as VDSM continues to indicate that the highest offset remains below the threshold, the {virt-product-fullname} {engine-name} knows that the logical volume in question has sufficient storage to continue operations.</p>
</div>
<div class="paragraph">
<p>When QEMU indicates that usage has risen to exceed the threshold limit, VDSM communicates to the {engine-name} that the disk image will soon reach the size of it&#8217;s logical volume. The {virt-product-fullname} {engine-name} requests that the SPM host extend the logical volume. This process can be repeated as long as the data storage domain for the data center has available space. When the data storage domain runs out of available free space, you must manually add storage capacity to expand it.</p>
</div>
</div>
<div class="sect2">
<h3 id="Logical_Volume_Extension">Logical Volume Extension</h3>
<div class="paragraph">
<p>The {virt-product-fullname} {engine-name} uses thin provisioning to overcommit the storage available in a storage pool, and allocates more storage than is physically available. Virtual machines write data as they operate. A virtual machine with a thinly-provisioned disk image will eventually write more data than the logical volume backing its disk image can hold. When this happens, logical volume extension is used to provide additional storage and facilitate the continued operations for the virtual machine.</p>
</div>
<div class="paragraph">
<p>{virt-product-fullname} provides a thin provisioning mechanism over LVM. When using QCOW2 formatted storage, {virt-product-fullname} relies on the host system process qemu-kvm to map storage blocks on disk to logical blocks in a sequential manner. This allows, for example, the definition of a logical 100 GB disk backed by a 1 GB logical volume. When qemu-kvm crosses a usage threshold set by VDSM, the local VDSM instance makes a request to the SPM for the logical volume to be extended by another one gigabyte. VDSM on the host running a virtual machine in need of volume extension notifies the SPM VDSM that more space is required. The SPM extends the logical volume and the SPM VDSM instance causes the host VDSM to refresh volume group information and recognize that the extend operation is complete. The host can continue operations.</p>
</div>
<div class="paragraph">
<p>Logical Volume extension does not require that a host know which other host is the SPM; it could even be the SPM itself. The storage extension communication is done via a storage mailbox. The storage mailbox is a dedicated logical volume on the data storage domain. A host that needs the SPM to extend a logical volume writes a message in an area designated to that particular host in the storage mailbox. The SPM periodically reads the incoming mail, performs requested logical volume extensions, and writes a reply in the outgoing mail. After sending the request, a host monitors its incoming mail for responses every two seconds. When the host receives a successful reply to its logical volume extension request, it refreshes the logical volume map in device mapper to recognize the newly allocated storage.</p>
</div>
<div class="paragraph">
<p>When the physical storage available to a storage pool is nearly exhausted, multiple images can run out of usable storage with no means to replenish their resources. A storage pool that exhausts its storage causes QEMU to return an <strong>enospc error</strong>, which indicates that the device no longer has any storage available. At this point, running virtual machines are automatically paused and manual intervention is required to add a new LUN to the volume group.</p>
</div>
<div class="paragraph">
<p>When a new LUN is added to the volume group, the Storage Pool Manager automatically distributes the additional storage to logical volumes that need it. The automatic allocation of additional resources allows the relevant virtual machines to automatically continue operations uninterrupted or resume operations if stopped.</p>
</div>
</div>
<div class="sect2">
<h3 id="The_Effect_of_Storage_Domain_Actions_on_Storage_Capacity">The Effect of Storage Domain Actions on Storage Capacity</h3>
<div class="dlist">
<dl>
<dt class="hdlist1">Power on, power off, and reboot a stateless virtual machine</dt>
<dd>
<p>These three processes affect the copy-on-write (COW) layer in a stateless virtual machine. For more information, see the <strong>Stateless</strong> row of the <a href="{URL_virt_product_docs}{URL_format}virtual_machine_management_guide/index#Virtual_Machine_General_settings_explained">Virtual Machine General Settings table</a> in the <em>Virtual Machine Management Guide</em>.</p>
</dd>
<dt class="hdlist1">Create a storage domain</dt>
<dd>
<p>Creating a block storage domain results in files with the same names as the seven LVs shown below, and initially should take less capacity.</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight nowrap"><code data-lang="Bash">ids              64f87b0f-88d6-49e9-b797-60d36c9df497 -wi-ao---- 128.00m
inbox            64f87b0f-88d6-49e9-b797-60d36c9df497 -wi-a----- 128.00m
leases           64f87b0f-88d6-49e9-b797-60d36c9df497 -wi-a-----   2.00g
master           64f87b0f-88d6-49e9-b797-60d36c9df497 -wi-ao----   1.00g
metadata         64f87b0f-88d6-49e9-b797-60d36c9df497 -wi-a----- 512.00m
outbox           64f87b0f-88d6-49e9-b797-60d36c9df497 -wi-a----- 128.00m
xleases          64f87b0f-88d6-49e9-b797-60d36c9df497 -wi-a-----   1.00g</code></pre>
</div>
</div>
</dd>
<dt class="hdlist1">Delete a storage domain</dt>
<dd>
<p>Deleting a storage domain frees up capacity on the disk by the same of amount of capacity the process deleted.</p>
</dd>
<dt class="hdlist1">Migrate a storage domain</dt>
<dd>
<p>Migrating a storage domain does not use additional storage capacity. For more information about migrating storage domains, see <a href="{URL_virt_product_docs}{URL_format}administration_guide/index#Migrating_SD_between_DC_Same_Env">Migrating Storage Domains Between Data Centers in the Same Environment</a> in the <em>Administration Guide</em>.</p>
</dd>
<dt class="hdlist1">Move a virtual disk to other storage domain</dt>
<dd>
<p>Migrating a virtual disk requires enough free space to be available on the target storage domain. You can see the target domain&#8217;s approximate free space in the Administration Portal.</p>
<div class="paragraph">
<p>The storage types in the move process affect the visible capacity. For example, if you move a preallocated disk from block storage to file storage, the resulting free space may be considerably smaller than the initial free space.</p>
</div>
<div class="paragraph">
<p>Live migrating a virtual disk to another storage domain also creates a snapshot, which is automatically merged after the migration is complete. To learn more about moving virtual disks, see <a href="{URL_virt_product_docs}{URL_format}administration_guide/index#Moving_a_Virtual_Disk">Moving a Virtual Disk</a> in the <em>Administration Guide</em>.</p>
</div>
</dd>
<dt class="hdlist1">Pause a storage domain</dt>
<dd>
<p>Pausing a storage domain does not use any additional storage capacity.</p>
</dd>
<dt class="hdlist1">Create a snapshot of a virtual machine</dt>
<dd>
<p>Creating a snapshot of a virtual machine can affect the storage domain capacity.</p>
<div class="ulist">
<ul>
<li>
<p>Creating a live snapshot uses memory snapshots by default and generates two additional volumes per virtual machine. The first volume is the sum of the memory, video memory, and 200 MB of buffer. The second volume contains the virtual machine configuration, which is several MB in size. When using block storage, rounding up occurs to the nearest unit {virt-product-fullname} can provide.</p>
</li>
<li>
<p>Creating an offline snapshot initially consumes 1 GB of block storage and is dynamic up to the size of the disk.</p>
</li>
<li>
<p>Cloning a snapshot creates a new disk the same size as the original disk.</p>
</li>
<li>
<p>Committing a snapshot removes all child volumes, depending on where in the chain the commit occurs.</p>
</li>
<li>
<p>Deleting a snapshot eventually removes the child volume for each disk and is only supported with a running virtual machine.</p>
</li>
<li>
<p>Previewing a snapshot creates a temporary volume per disk, so sufficient capacity must be available to allow the creation of the preview.</p>
</li>
<li>
<p>Undoing a snapshot preview removes the temporary volume created by the preview.</p>
</li>
</ul>
</div>
</dd>
<dt class="hdlist1">Attach and remove direct LUNs</dt>
<dd>
<p>Attaching and removing direct LUNs does not affect the storage domain since they are not a storage domain component. For more information, see <a href="{URL_virt_product_docs}{URL_format}administration_guide/index#Overview_of_Live_Storage_Migration">Overview of Live Storage Migration</a> in the <em>Administration Guide</em>.</p>
</dd>
</dl>
</div>
</div>
</div>
</div>



        </section>
      </section>
    </main>

    <script src="/ovirt-site/previews/2768/javascripts/lib/headings_anchors.js" type="text/javascript"></script><footer class='text-center' id='footer'><hr class='visible-print'><ul class='footer-nav-list'><li><a href='/ovirt-site/previews/2768/privacy-policy.html' target='_blank' title='Privacy policy'>Privacy policy</a></li><li><a href='/ovirt-site/previews/2768/community/about.html' target='_blank' title='About'>About</a></li><li><a href='/ovirt-site/previews/2768/general-disclaimer.html' target='_blank' title='Disclaimers'>Disclaimers</a></li></ul>&copy; 2013&ndash;2022 oVirt<div class='edit-this-page'><a href='https://github.com/oVirt/ovirt-site/issues/new?labels=documentation&amp;title=Issue:%20/documentation/doc-Technical_Reference/chap-Storage.html&amp;template=issue_template_documentation.md' target='_blank' title='Report an issue'><i class="icon fab fa-github"></i>Report an issue with this page</a></div><div class='edit-this-page'><a href='https://github.com/oVirt/ovirt-site/edit/master/source/documentation/doc-Technical_Reference/chap-Storage.adoc' target='_blank' title='Edit this page'><i class="icon fab fa-github"></i>Edit this page</a></div></footer></body>

</html>
