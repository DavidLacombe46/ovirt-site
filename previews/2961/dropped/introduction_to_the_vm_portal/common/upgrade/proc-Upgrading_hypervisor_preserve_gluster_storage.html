<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>oVirt | oVirt is a free open-source virtualization solution for your entire enterprise</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="oVirt" />
<meta property="og:locale" content="en" />
<meta name="description" content="oVirt is a free open-source virtualization solution for your entire enterprise" />
<meta property="og:description" content="oVirt is a free open-source virtualization solution for your entire enterprise" />
<link rel="canonical" href="https://ovirt.github.io/ovirt-site/previews/2961/dropped/introduction_to_the_vm_portal/common/upgrade/proc-Upgrading_hypervisor_preserve_gluster_storage.html" />
<meta property="og:url" content="https://ovirt.github.io/ovirt-site/previews/2961/dropped/introduction_to_the_vm_portal/common/upgrade/proc-Upgrading_hypervisor_preserve_gluster_storage.html" />
<meta property="og:site_name" content="oVirt" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="oVirt" />
<meta name="twitter:site" content="@ovirt" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"oVirt is a free open-source virtualization solution for your entire enterprise","headline":"oVirt","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://ovirt.github.io/ovirt-site/previews/2961/images/logo.svg"}},"url":"https://ovirt.github.io/ovirt-site/previews/2961/dropped/introduction_to_the_vm_portal/common/upgrade/proc-Upgrading_hypervisor_preserve_gluster_storage.html"}</script>
<!-- End Jekyll SEO tag -->

  <meta name="viewport" content="initial-scale=1.0,user-scalable=no,maximum-scale=1,width=device-width">
  <link rel="stylesheet" href="/ovirt-site/previews/2961/stylesheets/fonts.css" />
  <link rel='stylesheet' href='https://fonts.googleapis.com/css?family=Open+Sans'>
  <link rel="stylesheet" href="/ovirt-site/previews/2961/stylesheets/application.css" />
  <link rel="stylesheet" href="/ovirt-site/previews/2961/stylesheets/print.css" media="print" />
  <link rel="stylesheet" href="/ovirt-site/previews/2961/stylesheets/coderay.css" media="screen" />
  <link rel="stylesheet" href="/ovirt-site/previews/2961/stylesheets/asciidoc.css" />
  <script src="/ovirt-site/previews/2961/javascripts/vendor/jquery.js" type="text/javascript"></script>
  <script src="/ovirt-site/previews/2961/javascripts/vendor/bootstrap.min.js" type="text/javascript"></script>
  <script src="/ovirt-site/previews/2961/javascripts/vendor/bootstrap-sortable.js" type="text/javascript"></script>
  <script src="/ovirt-site/previews/2961/javascripts/vendor/moment.js" type="text/javascript"></script>
  <script src="/ovirt-site/previews/2961/javascripts/vendor/fullcalendar/fullcalendar.js" type="text/javascript"></script>
  <script src="/ovirt-site/previews/2961/javascripts/lib/cal-widget.js" type="text/javascript"></script>
  <link href='/ovirt-site/previews/2961/favicon.ico' rel='shortcut icon' sizes='36x36 24x24 16x16' type='image/x-icon'/>
<link href='/ovirt-site/previews/2961/favicon.png' rel='icon' sizes='196x196' type='image/png'/>
<meta content='/ovirt-site/previews/2961/mstile-icon-128x128.png' name='msapplication-TileImage'/>
<meta content='transparent' name='msapplication-TileColor'/>
<link href='/ovirt-site/previews/2961/manifest.webmanifest' rel='manifest'/>
<meta content='/ovirt-site/previews/2961/browserconfig.xml' name='msapplication-config'/>
</head>
<body class=""><header class='masthead hidden-print' id='branding' role='banner'><section class='hgroup'></section><div id='access'><nav id="mainNav" class="navbar navbar-fixed-top affix-top">  <div class="container">    <div class="col-sm-2 navbar-header">      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#menu_id0980fddf">        <span class="sr-only">Toggle navigation</span>        <span>Menu</span>        <span class="fa fa-bars"></span>      </button>      <a class="navbar-brand" href="/ovirt-site/previews/2961/">        <img id="logo" alt="oVirt" src="/ovirt-site/previews/2961/images/logo.svg">      </a>    </div>    <!-- Collect the nav links, forms, and other content for toggling -->    <div class="col-sm-10">      <div class="navbar-collapse collapse" id="menu_id0980fddf">        <ul class="nav navbar-nav">          <li class="hidden active">            <a href="#page-wrap"></a>          </li><li role='menuitem'>  <a href='/ovirt-site/previews/2961/download/'>Download</a></li><li role='menuitem'>  <a href='/ovirt-site/previews/2961/documentation/'>Documentation</a></li><li role='menuitem'>  <a href='/ovirt-site/previews/2961/develop/'>Developers</a></li><li role='menuitem'>  <a href='/ovirt-site/previews/2961/community/'>Community</a></li><li role='menuitem'>  <a href='/ovirt-site/previews/2961//lists.ovirt.org/archives/'>Forum</a></li><li role='menuitem'>  <a href='/ovirt-site/previews/2961//blogs.ovirt.org/'>Blog</a></li>              </ul>          </div>          <!-- /.navbar-collapse -->      </div>      <!-- /.container-fluid -->    </div>  </div></nav></div></header><main id="page-wrap" class="page-wrap" aria-label="Content">
      <section id="page" class="page">
        <!-- adapted from https://github.com/git-no/jekyll-breadcrumbs -->
<nav class="breadcrumbs bootstrap hidden-sm-down" aria-label="breadcrumb">

   <ol class="breadcrumb list-unstyled" vocab="http://schema.org/" typeof="BreadcrumbList">

      
        
        
        <li class="breadcrumb-item" property="itemListElement" typeof="ListItem">
           <a property="item" typeof="WebPage" href="/ovirt-site/previews/2961/">
              <span property="name"></span>
              <meta property="position" content="1" />
           </a>
        </li>
      
        
        
        <li class="breadcrumb-item" property="itemListElement" typeof="ListItem">
           <a property="item" typeof="WebPage" href="/ovirt-site/previews/2961/dropped/">
              <span property="name">Dropped</span>
              <meta property="position" content="2" />
           </a>
        </li>
      
        
        
        <li class="breadcrumb-item" property="itemListElement" typeof="ListItem">
           <a property="item" typeof="WebPage" href="/ovirt-site/previews/2961/dropped/introduction_to_the_vm_portal/">
              <span property="name">Introduction to the VM Portal</span>
              <meta property="position" content="3" />
           </a>
        </li>
      
        
        
        <li class="breadcrumb-item" property="itemListElement" typeof="ListItem">
           <a property="item" typeof="WebPage" href="/ovirt-site/previews/2961/dropped/introduction_to_the_vm_portal/common/">
              <span property="name">Common</span>
              <meta property="position" content="4" />
           </a>
        </li>
      
        
        
        <li class="breadcrumb-item" property="itemListElement" typeof="ListItem">
           <a property="item" typeof="WebPage" href="/ovirt-site/previews/2961/dropped/introduction_to_the_vm_portal/common/upgrade/">
              <span property="name">Upgrade</span>
              <meta property="position" content="5" />
           </a>
        </li>
      
        
        
          

   </ol>

</nav>

        
          
        
        <section id="content" class="content container">
          
<div class="paragraph">
<p>Environments with Gluster as storage can take a backup of Gluster storage and be restored after the {hypervisor-shortname} upgrade.
Try to keep workloads on all virtual machines using Gluster storage as light as possible to shorten the time required to upgrade. If there are highly write-intensive workloads, expect more time to restore.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>If there are geo-replication schedules on the storage domains, remove those schedules to avoid upgrade conflicts.</p>
</li>
<li>
<p>No geo-replication sync are currently running.</p>
</li>
<li>
<p>Additional disk space of 100 GB is required on 3 hosts for creating a new volume for the new {hypervisor-shortname} 4.4 {engine-name} deployment.</p>
</li>
<li>
<p>All data centers and clusters in the environment must have a cluster compatibility level of 4.3 before you start the procedure.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Restriction</div>
<ul>
<li>
<p>Network-Bound Disk Encryption (NBDE) is supported only with new deployments with {virt-product-fullname} 4.4. This feature cannot be enabled during the upgrade.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Create a new Gluster volume for {hypervisor-shortname} 4.4 {engine-name} deployment.</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Create a new brick on each host for the new {hypervisor-shortname} 4.4 self-hosted engine virtual machine(VM).</p>
</li>
<li>
<p>If you have a spare disk in the setup, follow the document Create Volume from the web console.</p>
</li>
<li>
<p>If there is enough space for a new {engine-name} 100GB brick in the existing Volume Group(VG), it can be used as a new {engine-name} Logical Volume (LV).</p>
<div class="paragraph">
<p>Run the following commands on all the hosts, unless specified otherwise explicitly:</p>
</div>
</li>
<li>
<p>Check the free size of the Volume Group (VG).</p>
<div class="listingblock">
<div class="content">
<pre># vgdisplay &lt;VG_NAME&gt; | grep -i free</pre>
</div>
</div>
</li>
<li>
<p>Create one more Logical Volume in this VG.</p>
<div class="listingblock">
<div class="content">
<pre># lvcreate -n gluster_lv_newengine -L 100G &lt;EXISTING_VG&gt;</pre>
</div>
</div>
</li>
<li>
<p>Format the new Logical Volume (LV) as XFS.</p>
<div class="listingblock">
<div class="content">
<pre># mkfs.xfs  &lt;LV_NAME&gt;</pre>
</div>
</div>
</li>
<li>
<p>Create the mount point for the new brick.</p>
<div class="listingblock">
<div class="content">
<pre># mkdir /gluster_bricks/newengine</pre>
</div>
</div>
</li>
<li>
<p>Create an entry corresponding to the newly created filesystem in
<code>/etc/fstab</code> and mount the filesystem.</p>
</li>
<li>
<p>Set the SELinux Labels on the brick mount points.</p>
<div class="listingblock">
<div class="content">
<pre># semanage fcontext -a -t glusterd_brick_t /gluster_bricks/newengine
 restorecon -Rv /gluster_bricks/newengine</pre>
</div>
</div>
</li>
<li>
<p>Create a new gluster volume by executing the gluster command on one of the hosts in the cluster:</p>
<div class="listingblock">
<div class="content">
<pre># gluster volume create newengine replica 3 host1:/gluster_bricks/newengine/newengine host2:/gluster_bricks/newengine/newengine host3:/gluster_bricks/newengine/newengine</pre>
</div>
</div>
</li>
<li>
<p>Set the required volume options on the newly created volume. Run the following commands on one of the hosts in the cluster:</p>
<div class="listingblock">
<div class="content">
<pre># gluster volume set newengine group virt
 gluster volume set newengine network.ping-timeout 30
 gluster volume set newengine cluster.granular-entry-heal enable
 gluster volume set newengine network.remote-dio off
 gluster volume set newengine performance.strict-o-direct on
 gluster volume set newengine storage.owner-uid 36
 gluster volume set newengine storage.owner-gid 36</pre>
</div>
</div>
</li>
<li>
<p>Start the newly created Gluster volume. Run the following command on one of the hosts in the cluster.</p>
<div class="listingblock">
<div class="content">
<pre># gluster volume start newengine</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Backup the Gluster configuration on all {hypervisor-shortname} 4.3 nodes using the backup playbook.</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>The backup playbook is available with the latest version of {hypervisor-shortname} 4.3. If this playbook is not available, create a playbook and inventory file:</p>
<div class="literalblock">
<div class="content">
<pre>/etc/ansible/roles/gluster.ansible/playbooks/hc-ansible-deployment/archive_config.yml</pre>
</div>
</div>
<div class="paragraph">
<p>Example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre> all:
  hosts:
    host1:
    host2:
    host3:
  vars:
    backup_dir: /archive
    nbde_setup: false
    upgrade: true</pre>
</div>
</div>
</li>
<li>
<p>Edit the backup inventory file with correct details.</p>
<div class="listingblock">
<div class="content">
<pre>  Common variables
  backup_dir -&gt;  Absolute path to directory that contains the extracted contents of the backup archive
  nbde_setup -&gt; Set to false as the {virt-product-fullname} 4.3 setup doesn’t support NBDE
  upgrade -&gt; Default value true . This value will make no effect with backup</pre>
</div>
</div>
</li>
<li>
<p>Switch to the directory and execute the playbook.</p>
<div class="listingblock">
<div class="content">
<pre>ansible-playbook -i archive_config_inventory.yml archive_config.yml --tags backupfiles</pre>
</div>
</div>
</li>
<li>
<p>The generated backup configuration tar file is generated under /root with the name <code>{hypervisor-shortname}-&lt;HOSTNAME&gt;-backup.tar.gz</code>. On all the hosts, copy the backup configuration tar file to the backup host.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Using the Manager Administration Portal, migrate the VMs running on the first host to other hosts in the cluster.</p>
</li>
<li>
<p>Backup {engine-name} configurations.</p>
<div class="listingblock">
<div class="content">
<pre># engine-backup --mode=backup --scope=all --file=&lt;backup-file.tar.gz&gt; --log=&lt;logfile&gt;</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Before creating a backup, do the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Enable  <strong>Global Maintenance</strong> for the self-hosted engine(SHE).</p>
</li>
<li>
<p>Log in to the {engine-name} VM using SSH and stop the ovirt-engine service.</p>
</li>
<li>
<p>Copy the backup file from the self-hosted engine VM to the remote host.</p>
</li>
<li>
<p>Shut down the {engine-name}.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Check for any pending self-heal tasks on all the replica 3 volumes. Wait for the heal to be completed.</p>
</li>
<li>
<p>Run the following command on one of the hosts:</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="terminal"># gluster volume heal &lt;volume&gt; info summary</code></pre>
</div>
</div>
</li>
<li>
<p>Stop the <code>glusterfs</code> brick process and unmount all the bricks on the first host to maintain file system consistency. Run the following on the first host:</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="terminal"># pkill glusterfsd; pkill glusterfs
# systemctl stop glusterd
# umount /gluster_bricks/*</code></pre>
</div>
</div>
</li>
<li>
<p>Reinstall the host with {hypervisor-shortname} 4.4 ISO, only formatting the OS disk.</p>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Make sure that the installation does not format the other disks, as bricks are created on top of those disks.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Once the node is up following the {hypervisor-shortname} 4.4 installation reboot, subscribe to {hypervisor-shortname} 4.4 repos as outlined in the Installation Guide, or install the downloaded {hypervisor-shortname} 4.4 appliance.</p>
<div class="listingblock">
<div class="content">
<pre># yum install &lt;appliance&gt;</pre>
</div>
</div>
</li>
<li>
<p>Disable the devices used for Gluster bricks.</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Create the new SSH private and public key pairs.</p>
</li>
<li>
<p>Establish SSH public key authentication ( passwordless SSH ) to the same host, using frontend and backend network FQDN.</p>
</li>
<li>
<p>Create the inventory file:</p>
<div class="literalblock">
<div class="content">
<pre>/etc/ansible/roles/gluster.ansible/playbooks/hc-ansible-deployment/blacklist_inventory.yml</pre>
</div>
</div>
<div class="paragraph">
<p>Example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre> hc_nodes:
  hosts:
    host1-backend-FQDN.example.com:
      blacklist_mpath_devices:
         - sda
         - sdb</pre>
</div>
</div>
</li>
<li>
<p>Run the playbook</p>
<div class="listingblock">
<div class="content">
<pre>ansible-playbook -i blacklist_inventory.yml /etc/ansible/roles/gluster.ansible/playbooks/hc-ansible-deployment/tasks/gluster_deployment.yml --tags blacklistdevices*</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Copy the {engine-name} backup and host config tar files from the backup host to the newly installed host and untar the content using scp.</p>
</li>
<li>
<p>Restore the Gluster configuration files.</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Extract the contents of the Gluster configuration files</p>
<div class="listingblock">
<div class="content">
<pre> # mkdir /archive
 # tar -xvf /root/ovirt-host-host1.example.com.tar.gz -C /archive/</pre>
</div>
</div>
</li>
<li>
<p>Edit the inventory file to perform restoration of the configuration files. The Inventory file is available at <code>/etc/ansible/roles/gluster.ansible/playbooks/hc-ansible-deployment/archive_config_inventory.yml</code></p>
<div class="paragraph">
<p>Example playbook content:</p>
</div>
<div class="listingblock">
<div class="content">
<pre> all:
   hosts:
 	host1.example.com:
   vars:
 	backup_dir: /archive
 	nbde_setup: false
 	upgrade: true</pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="literalblock">
<div class="content">
<pre>Use only one host under ‘hosts’ section of restoration playbook.</pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Execute the playbook to restore configuration files</p>
<div class="listingblock">
<div class="content">
<pre>ansible-playbook -i archive_config_inventory.yml archive_config.yml --tags restorefiles</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Perform {engine-name} deployment with the option <code>--restore-from-file</code> pointing to the backed-up archive from the {engine-name}. This {engine-name} deployment can be done interactively using the <code>hosted-engine --deploy</code> command, providing the storage corresponds to the newly created {engine-name} volume. The same can also be done using <code>ovirt-ansible-hosted-engine-setup</code> in an automated procedure.
The following procedure is an automated method for deploying a HostedEngine VM using the backup:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Create a playbook for HostedEngine deployment in the newly installed host:</p>
<div class="paragraph">
<p><code>/etc/ansible/roles/gluster.ansible/playbooks/hc-ansible-deployment/he.yml</code></p>
</div>
<div class="listingblock">
<div class="content">
<pre>- name: Deploy oVirt hosted engine
  hosts: localhost
  roles:
    - role: ovirt.hosted_engine_setup</pre>
</div>
</div>
</li>
<li>
<p>Update the HostedEngine related information using the template file:</p>
<div class="paragraph">
<p><code>/etc/ansible/roles/gluster.ansible/playbooks/hc-ansible-deployment/he_gluster_vars.json</code></p>
</div>
<div class="paragraph">
<p>Example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="terminal"># cat /etc/ansible/roles/gluster.ansible/playbooks/hc-ansible-deployment/he_gluster_vars.json

{
  &quot;he_appliance_password&quot;: &quot;&lt;password&gt;&quot;,
  &quot;he_admin_password&quot;: &quot;&lt;password&gt;&quot;,
  &quot;he_domain_type&quot;: &quot;glusterfs&quot;,
  &quot;he_fqdn&quot;: &quot;&lt;hostedengine.example.com&gt;&quot;,
  &quot;he_vm_mac_addr&quot;: &quot;&lt;00:18:15:20:59:01&gt;&quot;,
  &quot;he_default_gateway&quot;: &quot;&lt;19.70.12.254&gt;&quot;,
  &quot;he_mgmt_network&quot;: &quot;ovirtmgmt&quot;,
  &quot;he_storage_domain_name&quot;: &quot;HostedEngine&quot;,
  &quot;he_storage_domain_path&quot;: &quot;&lt;/newengine&gt;&quot;,
  &quot;he_storage_domain_addr&quot;: &quot;&lt;host1.example.com&gt;&quot;,
  &quot;he_mount_options&quot;: &quot;backup-volfile-servers=&lt;host2.example.com&gt;:&lt;host3.example.com&gt;&quot;,
  &quot;he_bridge_if&quot;: &quot;&lt;eth0&gt;&quot;,
  &quot;he_enable_hc_gluster_service&quot;: true,
  &quot;he_mem_size_MB&quot;: &quot;16384&quot;,
  &quot;he_cluster&quot;: &quot;Default&quot;,
  &quot;he_restore_from_file&quot;: &quot;/root/engine-backup.tar.gz&quot;,
  &quot;he_vcpus&quot;: 4
}</code></pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="ulist">
<ul>
<li>
<p>In the above he_gluster_vars.json, There are 2 important values: “he_restore_from_file” and “he_storage_domain_path”. The first option “he_restore_from_file” should point to the absolute file name of the {engine-name} backup archive copied to the local machine. The second option “he_storage_domain_path” should refer to the newly created Gluster volume.</p>
</li>
<li>
<p>Also note that the previous version of {hypervisor-shortname} Version running inside the {engine-name} VM is down and that will be discarded.  MAC Address and FQDN corresponding to the older {engine-name} VM can be reused for the new {engine-name} as well.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>For static {engine-name} network configuration, add more options as listed below:</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="terminal">  “he_vm_ip_addr”:  “&lt;engine VM ip address&gt;”
  “he_vm_ip_prefix”:  “&lt;engine VM ip prefix&gt;”
  “he_dns_addr”:  “&lt;engine VM DNS server&gt;”
  “he_default_gateway”:  “&lt;engine VM default gateway&gt;”</code></pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If there is no specific DNS available, try to include 2 more options:
“he_vm_etc_hosts”: true
and
“he_network_test”: “ping”</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Run the playbook to deploy HostedEngine Deployment.</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="terminal"># cd /etc/ansible/roles/gluster.ansible/playbooks/hc-ansible-deployment
# ansible-playbook he.yml --extra-vars &quot;@he_gluster_vars.json&quot;</code></pre>
</div>
</div>
</li>
<li>
<p>Wait for the self-hosted engine deployment to complete.</p>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If there are any failures during self-hosted engine deployment, find the problem looking at the log messages under <code>/var/log/ovirt-hosted-engine-setup</code>, fix the problem. Clean the failed self-hosted engine deployment using the command <code>ovirt-hosted-engine-cleanup</code> and rerun the deployment.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Log in to the {hypervisor-shortname} 4.4 Administration Portal on the newly installed {virt-product-fullname} manager. Make sure all the hosts are in the ‘up’ state, and wait for the self-heal on the Gluster volumes to be completed.</p>
</li>
<li>
<p>Upgrade the next host</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Move the next host (ideally, the next one in order), to Maintenance mode from the Administration Portal. Stop the Gluster service while moving this host to Maintenance mode.</p>
</li>
<li>
<p>From the command line of the host, unmount Gluster bricks</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="terminal"># umount /gluster_bricks/*</code></pre>
</div>
</div>
</li>
<li>
<p>Reinstall this host with {hypervisor-shortname} 4.4.</p>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Make sure that the installation does not format the other disks, as bricks are created on top of those disks.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>If multipath configuration is not available on the newly installed host, disable the Gluster devices. The inventory file is already created in the first host as part of the step <em>Disable the devices used for Gluster bricks</em>.</p>
<div class="olist lowerroman">
<ol class="lowerroman" type="i">
<li>
<p>Set up SSH public key authentication from the first host to the newly installed host.</p>
</li>
<li>
<p>Update the inventory with the new host name.</p>
</li>
<li>
<p>Execute the playbook.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Copy the Gluster configuration tar files from the backup host to the newly installed host and untar the content.</p>
</li>
<li>
<p>Restore Gluster configuration on the newly installed host by executing the playbook as described in the step <em>Restoring the Gluster configurations files</em> on this host.</p>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Edit the  playbook on the newly installed host and execute it as described in the step <em>Perform manager deployment with the option --restore-from-file&#8230;&#8203;</em>. Do not change hostname and execute on the same host.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Reinstall the host in {hypervisor-shortname} Administration Portal Copy the authorized key from the first deployed host in {hypervisor-shortname} 4.4</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="terminal"># scp <a href="mailto:root@host1.example.com">root@host1.example.com</a>:/root/.ssh/authorized_keys /root/.ssh/</code></pre>
</div>
</div>
<div class="olist lowerroman">
<ol class="lowerroman" type="i">
<li>
<p>In the <strong>Administration Portal</strong>, The host will be in ‘Maintenance’. Go to <span class="menuseq"><b class="menu">Compute</b>&#160;<i class="fa fa-angle-right caret"></i> <b class="submenu">Hosts</b>&#160;<i class="fa fa-angle-right caret"></i> <b class="submenu">Installation</b>&#160;<i class="fa fa-angle-right caret"></i> <b class="menuitem">Reinstall</b></span>.</p>
</li>
<li>
<p>In the <strong>New Host</strong> dialog box <strong>HostedEngine</strong> tab, and select the <strong>deploy</strong> self-hosted engine deployment action.</p>
</li>
<li>
<p>Wait for the host to reach <strong>Up</strong> status.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Make sure that there are no errors in the volumes related to GFID mismatch. If there are any errors, resolve them.</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="terminal">grep -i &quot;gfid mismatch&quot; /var/log/glusterfs/*</code></pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Repeat the step <em>Upgrade the next host</em>  for all the {hypervisor-shortname} in the cluster.</p>
</li>
<li>
<p><strong>(optional)</strong> If a separate Gluster logical network exists in the cluster,  attach the Gluster logical network to the required interface on each host.</p>
</li>
<li>
<p>Remove the old {engine-name} storage domain. Identify the old {engine-name} storage domain by the name <strong>hosted_storage</strong> with no gold star next to it, listed under <span class="menuseq"><b class="menu">Storage</b>&#160;<i class="fa fa-angle-right caret"></i> <b class="menuitem">Domains</b></span>.</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Go to the <span class="menuseq"><b class="menu">Storage</b>&#160;<i class="fa fa-angle-right caret"></i> <b class="submenu">Domains</b>&#160;<i class="fa fa-angle-right caret"></i> <b class="submenu">hosted_storage</b>&#160;<i class="fa fa-angle-right caret"></i> <b class="menuitem">Data center</b></span> tab, and select <strong>Maintenance</strong>.</p>
</li>
<li>
<p>Wait for the storage domain to move into Maintenance mode.</p>
</li>
<li>
<p>Once the storage domain moves into Maintenance mode, click <b class="button">Detach</b>, the storage domain will move to <strong>unattached</strong>.</p>
</li>
<li>
<p>Select the unattached storage domain, click <b class="button">Remove</b>, and confirm <b class="button">OK</b>.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Stop and remove the old {engine-name} volume.</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Go to <span class="menuseq"><b class="menu">Storage</b>&#160;<i class="fa fa-angle-right caret"></i> <b class="menuitem">Volumes</b></span>, and select the old {engine-name} volume. Click <b class="button">Stop</b>, and confirm <b class="button">OK</b>.</p>
</li>
<li>
<p>Select the same volume, click <b class="button">Remove</b>, and confirm <b class="button">OK</b>.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Update the cluster compatibility version.</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Go to <span class="menuseq"><b class="menu">Compute</b>&#160;<i class="fa fa-angle-right caret"></i> <b class="menuitem">Clusters</b></span> and select the cluster <strong>Default</strong>, click <b class="button">Edit</b>, update the <strong>Compatibility Version</strong> to 4.4 and click <b class="button">OK</b>.</p>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="paragraph">
<p>There will be a warning for changing compatibility version, which requires VMs on the cluster to be restarted. Click <b class="button">OK</b>to confirm.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>There are new Gluster volume options available with {hypervisor-shortname} 4.4, apply those volume options on all the volumes. Execute the following on one of the nodes in the cluster:</p>
<div class="listingblock">
<div class="content">
<pre class="CodeRay highlight"><code data-lang="terminal"># for vol in <code>gluster volume list</code>; do gluster volume set $vol group virt; done</code></pre>
</div>
</div>
</li>
<li>
<p>Remove the archives and extracted the contents of the backup configuration files on all nodes.</p>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Creating an additional Gluster volume using the Web Console</div>
<ol class="arabic">
<li>
<p>Log in to the {engine-name} web console.</p>
</li>
<li>
<p>Go to <span class="menuseq"><b class="menu">Virtualization</b>&#160;<i class="fa fa-angle-right caret"></i> <b class="menuitem">Hosted Engine</b></span> and click <b class="button">Manage Gluster</b>.</p>
</li>
<li>
<p>Click <b class="button">Create Volume</b>.
In the Create Volume window, do the following:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>In the <strong>Hosts</strong> tab, select three different <code>ovirt-ng-nodes</code> with unused disks and click <b class="button">Next</b>.</p>
</li>
<li>
<p>In the <strong>Volumes</strong> tab, specify the details of the volume you want to create and click <b class="button">Next</b>.</p>
</li>
<li>
<p>In the <strong>Bricks</strong> tab, specify the details of the disks to be used to create the volume and click <b class="button">Next</b>.</p>
</li>
<li>
<p>In the <strong>Review</strong> tab, check the generated configuration file for any incorrect information. When you are satisfied, click <b class="button">Deploy</b>.</p>
</li>
</ol>
</div>
</li>
</ol>
</div>



        </section>
      </section>
    </main>

    <script src="/ovirt-site/previews/2961/javascripts/lib/headings_anchors.js" type="text/javascript"></script><footer class='text-center' id='footer'><hr class='visible-print'><ul class='footer-nav-list'><li><a href='/ovirt-site/previews/2961/privacy-policy.html' target='_blank' title='Privacy policy'>Privacy policy</a></li><li><a href='/ovirt-site/previews/2961/community/about.html' target='_blank' title='About'>About</a></li><li><a href='/ovirt-site/previews/2961/general-disclaimer.html' target='_blank' title='Disclaimers'>Disclaimers</a></li></ul>&copy; 2013&ndash;2022 oVirt<div class='edit-this-page'><a href='https://github.com/oVirt/ovirt-site/issues/new?labels=content&amp;title=Issue:%20/dropped/introduction_to_the_vm_portal/common/upgrade/proc-Upgrading_hypervisor_preserve_gluster_storage.html&amp;template=' target='_blank' title='Report an issue'><i class="icon fab fa-github"></i>Report an issue with this page</a></div><div class='edit-this-page'><a href='https://github.com/oVirt/ovirt-site/edit/master/source/dropped/introduction_to_the_vm_portal/common/upgrade/proc-Upgrading_hypervisor_preserve_gluster_storage.adoc' target='_blank' title='Edit this page'><i class="icon fab fa-github"></i>Edit this page</a></div></footer></body>

</html>
